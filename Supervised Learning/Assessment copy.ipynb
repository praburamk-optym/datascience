{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7445cf9-2573-47f9-8442-6951d04345e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART A DOMAIN: Medical\n",
    "# 1. Data Understanding:\n",
    "# import module\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b19383-1b10-49ca-84e5-98d5b05c3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Part 1. 1 a - Read all the 3 CSV files as DataFrame and store them into 3 separate variables.\n",
    "# read dataset\n",
    "normal_df = pd.read_csv(\"/Users/praburam.krishnamurthy/GL/Supervised Learning/Normal.csv\")\n",
    "normal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6c5b3-12ab-4998-b95c-8cfc9b84e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "h_df = pd.read_csv(\"/Users/praburam.krishnamurthy/GL/Supervised Learning/Type_H.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e31655-60ee-4d71-91af-92f2554fb5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "s_df = pd.read_csv(\"/Users/praburam.krishnamurthy/GL/Supervised Learning/Type_S.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec906b-bfa0-4613-a485-f6502d952990",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf4ed5-0380-4618-9f0b-2c13930e8697",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Part 1.1 b - Print Shape and columns of all the 3 DataFrames. \n",
    "shape = normal_df.shape\n",
    " \n",
    "# printing shape\n",
    "print(\"Shape = {}\".format(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd85772-ace2-4a10-bf7e-e329b95220ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Part 1.1 b - Print Shape and columns of all the 3 DataFrames. \n",
    "shape = h_df.shape\n",
    " \n",
    "# printing shape\n",
    "print(\"Shape = {}\".format(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b99a0c-9c0f-45f1-9b3f-3611c1e217f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Part 1.1 b - Print Shape and columns of all the 3 DataFrames. \n",
    "shape = s_df.shape\n",
    " \n",
    "# printing shape\n",
    "print(\"Shape = {}\".format(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf0af1b-c237-4235-99f9-655b9f453630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1.1 c - Compare Column names of all the 3 DataFrames and clearly write observations.\n",
    "import pandas as pd\n",
    "\n",
    "# Get column names of each DataFrame\n",
    "df1_columns = normal_df.columns\n",
    "df2_columns = h_df.columns\n",
    "df3_columns = s_df.columns\n",
    "\n",
    "# Convert column names to sets for easy comparison\n",
    "df1_set = set(df1_columns)\n",
    "df2_set = set(df2_columns)\n",
    "df3_set = set(df3_columns)\n",
    "\n",
    "# Compare column names across DataFrames\n",
    "common_columns = df1_set.intersection(df2_set, df3_set)\n",
    "unique_columns_df1 = df1_set - df2_set.union(df3_set)\n",
    "unique_columns_df2 = df2_set - df1_set.union(df3_set)\n",
    "unique_columns_df3 = df3_set - df1_set.union(df2_set)\n",
    "\n",
    "# Print observations\n",
    "print(\"Common Columns:\", common_columns)\n",
    "print(\"Unique Columns in DataFrame 1:\", unique_columns_df1)\n",
    "print(\"Unique Columns in DataFrame 2:\", unique_columns_df2)\n",
    "print(\"Unique Columns in DataFrame 3:\", unique_columns_df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa62e2-50be-4eee-807e-c090b8e459e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e91b9-50ff-401d-b7c6-75d747d35e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa81b2-2254-42dd-9cc2-4f64ad4e7bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de024d-f669-4a34-81f7-1c0cc3881126",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h_df.columns.intersection(s_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd3bc1-a9f1-44c4-bd87-4194e8f59225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations:\n",
    "\n",
    "#Common Columns: These are the column names that are present in all three DataFrames. ['P_incidence', 'P_tilt', 'L_angle', 'S_slope', 'P_radius', 'S_Degree',\n",
    "#        'Class']\n",
    "# There are no unique column names either present in DF1 or DF2 or DF3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a520e9e5-1f14-4ac2-aeba-f6007aba14d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1.1 d - Print DataTypes of all the 3 DataFrames. \n",
    "# Print data types of DataFrame 1\n",
    "print(\"Data Types of DataFrame 1:\")\n",
    "print(normal_df.dtypes)\n",
    "print()\n",
    "\n",
    "# Print data types of DataFrame 2\n",
    "print(\"Data Types of DataFrame 2:\")\n",
    "print(h_df.dtypes)\n",
    "print()\n",
    "\n",
    "# Print data types of DataFrame 3\n",
    "print(\"Data Types of DataFrame 3:\")\n",
    "print(s_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae5dc35-d45b-4689-aee0-6cce11d87338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1.1 e - Observe and share variation in ‘Class’ feature of all the 3 DaraFrames.\n",
    "\n",
    "# Extract unique values of 'Class' feature from all DataFrames\n",
    "unique_classes_df1 = normal_df['Class'].unique()\n",
    "unique_classes_df2 = h_df['Class'].unique()\n",
    "unique_classes_df3 = s_df['Class'].unique()\n",
    "\n",
    "# Print the unique values for each DataFrame\n",
    "print(\"Unique Classes in DataFrame 1:\", unique_classes_df1)\n",
    "print(\"Unique Classes in DataFrame 2:\", unique_classes_df2)\n",
    "print(\"Unique Classes in DataFrame 3:\", unique_classes_df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8e731d-0222-4f11-9782-52c37e7d6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Preparation and Exploration: \n",
    "# Part 1.2 a - Unify all the variations in ‘Class’ feature for all the 3 DataFrames.\n",
    "\n",
    "# Normalize 'Class' feature in all DataFrames\n",
    "normal_df['Class'] = normal_df['Class'].str.lower()  # Convert to lowercase\n",
    "h_df['Class'] = h_df['Class'].str.lower()\n",
    "s_df['Class'] = s_df['Class'].str.lower()\n",
    "\n",
    "# Check unique values of 'Class' feature after normalization\n",
    "unique_classes_df1 = normal_df['Class'].unique()\n",
    "unique_classes_df2 = h_df['Class'].unique()\n",
    "unique_classes_df3 = s_df['Class'].unique()\n",
    "\n",
    "# Print the unique values for each DataFrame\n",
    "print(\"Unique Classes in DataFrame 1:\", unique_classes_df1)\n",
    "print(\"Unique Classes in DataFrame 2:\", unique_classes_df2)\n",
    "print(\"Unique Classes in DataFrame 3:\", unique_classes_df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee6768-2256-480b-afd9-971669a94598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1.2 b - Combine all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat([normal_df, h_df, s_df], ignore_index=True)\n",
    "\n",
    "# Check the shape of the combined DataFrame\n",
    "print(\"Shape of Combined DataFrame:\", combined_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77280b8-156c-4652-9f38-aeb3b8970143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1.2 c - Print 5 random samples of this DataFrame\n",
    "\n",
    "# Print 5 random samples\n",
    "print(combined_df.sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b83a2-25ee-4bca-ab5c-0dfe66e2a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1.2 d - Print Feature-wise percentage of Null values\n",
    "null_percentage = (combined_df.isnull().sum() / len(combined_df)) * 100\n",
    "\n",
    "# Print feature-wise percentage of null values\n",
    "print(\"Feature-wise percentage of Null values:\")\n",
    "print(null_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eece36c-ac97-464b-bdce-2bbc58c5b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1.2 e - Check 5-point summary of the new DataFrame.\n",
    "# Check 5-point summary of the new DataFrame\n",
    "summary_statistics = combined_df.describe()\n",
    "\n",
    "# Print the 5-point summary\n",
    "print(\"5-point summary of the DataFrame:\")\n",
    "print(summary_statistics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bfa56e-1c2e-462f-a853-ea334580f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Analysis:\n",
    "# Part 1.3 a - Visualize a heatmap to understand correlation between all features \n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ae8e5-ebb4-41d9-bf7c-9248635d49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1.3 a - Visualize a heatmap to understand correlation between all features \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Exclude non-numeric columns from the DataFrame\n",
    "numeric_df = combined_df.select_dtypes(include=['number'])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Add title\n",
    "plt.title('Correlation Heatmap of Features')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a969e29-1b10-43be-9b97-4ac697dff4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1.3 b - Share insights on correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a7b21-60e6-4b4b-a41c-2af8684c1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. Features having stronger correlation with correlation value.\n",
    "    # Stronger Correlation:\n",
    "    #     S_slope has stronger correlation with P_incidence with correlation value of 0.81(closer to 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c2a67-3851-4a12-9d13-b09c2dab3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Features having weaker correlation with correlation value\n",
    "    # Weaker Correlation:\n",
    "    #     S_slope has weaker correlation with P_radius with correlation value of -.0.34(closer to -0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef472b5-c5df-4a0f-83f0-15d1bd77311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1.3 c - Visualize a pairplot with 3 classes distinguished by colors and share insights.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'Class' is the column representing the classes in the combined DataFrame\n",
    "# 'hue' parameter in pairplot will color points according to their class\n",
    "\n",
    "# Set style\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "# Create pairplot\n",
    "sns.pairplot(combined_df, hue=\"Class\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad76ea61-9255-4d3b-a6da-4f67b1b1b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1.3 d - Visualize a jointplot for ‘P_incidence’ and ‘S_slope’ and share insights.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create jointplot\n",
    "sns.jointplot(x='P_incidence', y='S_slope', data=combined_df, kind='scatter')\n",
    "\n",
    "# Add title\n",
    "plt.title('Jointplot of P_incidence and S_slope')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19365a4-7f9e-4497-9cc2-c5707cda0608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1.3 e - Visualize a boxplot to check distribution of the features and share insights.\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style of seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create boxplot for all features\n",
    "plt.figure(figsize=(12, 6))  # Set the figure size\n",
    "sns.boxplot(data=combined_df, orient=\"h\")  # \"orient\" parameter specifies horizontal orientation\n",
    "plt.title('Boxplot of Features')  # Add title\n",
    "plt.xlabel('Value')  # Add label for x-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62495c-f254-4d5c-b5c4-b8863c0c5b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Building\n",
    "#Part 1.4 a - Split data into X and Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa70a00b-2d8a-49c6-a45d-8e4cad7b8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Class' is the target variable\n",
    "# X contains all features except 'Class', Y contains only 'Class'\n",
    "\n",
    "# Drop the 'Class' column from the combined DataFrame to create X\n",
    "X = combined_df.drop(columns=['Class'])\n",
    "\n",
    "# Extract the 'Class' column as Y\n",
    "Y = combined_df['Class']\n",
    "\n",
    "# Print the shapes of X and Y to verify\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23120b40-17ae-4185-ad8b-0f6a42f69334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1.4 b - Split data into train and test with 80:20 proportion.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets with an 80:20 proportion\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets to verify\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of Y_train:\", Y_train.shape)\n",
    "print(\"Shape of Y_test:\", Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac4d30-163b-4ba4-84de-afbf45223b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1.4 c - Train a Supervised Learning Classification base model using KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee20d6-0cd2-49e6-9948-126f36247d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# Train the KNN classifier on the training data\n",
    "knn_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "Y_pred = knn_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95268406-fb4e-414b-92c4-090d2c6331df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1.4 d - Print all the possible performance metrics for both train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95951a45-8de0-44ce-9dc7-34c74738a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculate performance metrics for train data\n",
    "train_report = classification_report(Y_train, knn_classifier.predict(X_train))\n",
    "\n",
    "# Calculate performance metrics for test data\n",
    "test_report = classification_report(Y_test, Y_pred)\n",
    "\n",
    "# Print performance metrics for train data\n",
    "print(\"Performance metrics for train data:\")\n",
    "print(train_report)\n",
    "\n",
    "# Print performance metrics for test data\n",
    "print(\"\\nPerformance metrics for test data:\")\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b973777-049e-4de9-9516-8c3aa71129af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Performance Improvement:\n",
    "# Part 1.5 a - Experiment with various parameters to improve performance of the base model. \n",
    "# We will use GridSearchCV to search for the best combination of parameters from the specified parameter grid. We then train the KNN classifier with the best parameters and evaluate its performance on both the train and test data. \n",
    "# Adjust the parameter grid and the range of values based on our specific dataset and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0071c33-dc28-47be-aac9-786fcb6e0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan'],\n",
    "    'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# Grid search to find the best parameters\n",
    "grid_search = GridSearchCV(knn_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the KNN classifier with the best parameters\n",
    "best_knn_classifier = KNeighborsClassifier(**best_params)\n",
    "best_knn_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions\n",
    "Y_pred_train = best_knn_classifier.predict(X_train)\n",
    "Y_pred_test = best_knn_classifier.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Best parameters found:\", best_params)\n",
    "print(\"Performance metrics for train data:\")\n",
    "print(classification_report(Y_train, Y_pred_train))\n",
    "print(\"\\nPerformance metrics for test data:\")\n",
    "print(classification_report(Y_test, Y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aaa2f4-1b18-44b8-9081-c3f1b3112f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1.5 b - Clearly showcase improvement in performance achieved.\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy of base model\n",
    "base_model_accuracy = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "# Calculate accuracy of tuned model\n",
    "tuned_model_accuracy = accuracy_score(Y_test, Y_pred_test)\n",
    "\n",
    "# Calculate improvement in accuracy\n",
    "accuracy_improvement = (tuned_model_accuracy - base_model_accuracy) / base_model_accuracy * 100\n",
    "\n",
    "# Print improvement in accuracy\n",
    "print(\"Improvement in accuracy: {:.2f}%\".format(accuracy_improvement))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d43c6e-bf37-4818-b7c9-7a8ab4794215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1.5 c - Clearly state which parameters contributed most to improve model performance.\n",
    "\n",
    "# Get the best parameters and their corresponding score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Print the best parameters and their score\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score (accuracy):\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdeeccb-cfd4-436e-81a0-cd13208d6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Part B DOMAIN: Banking, Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad9c5f-4598-4d44-92be-8b851d07f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Understanding and Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d16ad-3a0f-4b0e-86aa-4ec9fb352c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Part 2.1 a - Read both the Datasets ‘Data1’ and ‘Data 2’ as DataFrame and store them into two separate variables.\n",
    "# read dataset\n",
    "df1 = pd.read_csv(\"/Users/praburam.krishnamurthy/GL/Supervised Learning/Data1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1481f-9386-41ef-aeed-97300265611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a716b1d-b689-4027-bbfd-00568dcee906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"/Users/praburam.krishnamurthy/GL/Supervised Learning/Data2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be5c9e-240a-48a8-b803-aa924f1546d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ac200-fa24-4d5b-82cf-d705e3f722ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Part 2.1 b - Print shape and Column Names and DataTypes of both the Dataframes.\n",
    "# Print shape, column names, and data types of DataFrame 1\n",
    "print(\"DataFrame 1:\")\n",
    "print(\"Shape:\", df1.shape)\n",
    "print(\"Column Names:\", df1.columns.tolist())\n",
    "print(\"Data Types:\")\n",
    "print(df1.dtypes)\n",
    "print()\n",
    "\n",
    "# Print shape, column names, and data types of DataFrame 2\n",
    "print(\"DataFrame 2:\")\n",
    "print(\"Shape:\", df2.shape)\n",
    "print(\"Column Names:\", df2.columns.tolist())\n",
    "print(\"Data Types:\")\n",
    "print(df2.dtypes)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911ee07-c7ef-45fd-bf57-e458e6cf4845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2.1 c - Merge both the Dataframes on ‘ID’ feature to form a single DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a37a90-7b1d-4eba-8015-b1fd20ed7165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df1 and df2 are our DataFrames\n",
    "\n",
    "# Merge both DataFrames on the 'ID' feature\n",
    "merged_df = pd.merge(df1, df2, on='ID', how='inner')\n",
    "\n",
    "# Print the shape and the first few rows of the merged DataFrame to verify\n",
    "print(\"Shape of merged DataFrame:\", merged_df.shape)\n",
    "print(\"Columns of merged DataFrame:\", merged_df.columns.tolist())\n",
    "print(\"Data Types of merged DataFrame:\")\n",
    "print(merged_df.dtypes)\n",
    "print(\"First few rows of merged DataFrame:\")\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3425f91-8afc-4370-830c-9fb7751a1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2.1 d - Change Datatype of below features to ‘Object’ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca819b5-b052-4941-9a8c-65b15f5a7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of features to convert to 'object' dtype\n",
    "features_to_convert = ['CreditCard', 'InternetBanking', 'FixedDepositAccount', 'Security', 'Level', 'HiddenScore']\n",
    "\n",
    "# Convert specified features to 'object' dtype\n",
    "merged_df[features_to_convert] = merged_df[features_to_convert].astype('object')\n",
    "\n",
    "# Print the data types of the specified features after conversion\n",
    "print(\"Data Types of specified features after conversion:\")\n",
    "print(merged_df[features_to_convert].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a559519e-a974-4aaf-843f-466a8d2a892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Exploration and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c038c2-66a1-48a1-85cd-0a5ea80b6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2.2 a - Visualize distribution of Target variable ‘LoanOnCard’ and clearly share insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a74c1-f013-42d3-8d90-7c72bfe8414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create countplot for the target variable 'LoanOnCard'\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='LoanOnCard', data=merged_df)\n",
    "plt.title('Distribution of LoanOnCard')\n",
    "plt.xlabel('LoanOnCard')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d02ba-1220-4052-9d2c-6b25fd9a9967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.2 b - Check the percentage of missing values and impute if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8bd5a3-d53e-4543-a000-360453dfb277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each feature\n",
    "missing_percentage = (merged_df.isnull().sum() / len(merged_df)) * 100\n",
    "\n",
    "# Print the percentage of missing values for each feature\n",
    "print(\"Percentage of missing values for each feature:\")\n",
    "print(missing_percentage)\n",
    "\n",
    "# Impute missing values if necessary\n",
    "# For example, if you decide to impute missing values with the mean of the column:\n",
    "# merged_df.fillna(merged_df.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc6f13-2938-4815-8e94-53af35f9945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.2 c - Check for unexpected values in each categorical variable and impute with best suitable value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ed522-8418-437a-a95a-5d1ccfdebb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical variables\n",
    "categorical_variables = merged_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Loop through each categorical variable\n",
    "for column in categorical_variables:\n",
    "    # Check for unexpected values\n",
    "    unexpected_values = merged_df[column].unique()  # Get unique values\n",
    "    unexpected_values = [val for val in unexpected_values if val not in ['0', '1']]  # Filter out expected values\n",
    "    \n",
    "    # If there are unexpected values, impute them with the most frequent value\n",
    "    if unexpected_values:\n",
    "        print(\"Unexpected values found in '{}' variable: {}\".format(column, unexpected_values))\n",
    "        # Impute unexpected values with the most frequent value\n",
    "        most_frequent_value = merged_df[column].mode().values[0]\n",
    "        merged_df[column].replace(unexpected_values, most_frequent_value, inplace=True)\n",
    "        print(\"Imputed unexpected values with '{}'\".format(most_frequent_value))\n",
    "    else:\n",
    "        print(\"No unexpected values found in '{}' variable\".format(column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc9856b-dd32-4680-a5c4-09ed781f903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Preparation and model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f503ce84-6c80-4081-86bc-489310c13280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.3 a - Split data into X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce42ecde-7fab-4b09-a74c-07cebc143a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'ID' and 'ZipCode' columns\n",
    "merged_df.drop(['ID', 'ZipCode'], axis=1, inplace=True)\n",
    "\n",
    "# Split data into features (X) and target variable (Y)\n",
    "X = merged_df.drop('LoanOnCard', axis=1)  # Features\n",
    "Y = merged_df['LoanOnCard']  # Target variable\n",
    "\n",
    "# Print the shapes of X and Y to verify\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8134c34-6d3a-4ae0-8090-99655cdc66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.3 b - Split data into train and test. Keep 25% data reserved for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46776644-3b30-4c05-8039-2c39be4f4dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets with 25% reserved for testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets to verify\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of Y_train:\", Y_train.shape)\n",
    "print(\"Shape of Y_test:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10672f95-fcd1-43a2-bf4a-18b796e50d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2.3 c - Train a Supervised Learning Classification base model - Logistic Regression.\n",
    "import pandas as pd\n",
    "\n",
    "# Check for NaN values in actual labels\n",
    "print(pd.isnull(actual_labels).sum())\n",
    "\n",
    "# Check for NaN values in predicted labels\n",
    "print(pd.isnull(y_pred).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01196aef-6d37-41d8-9391-62939b82307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming X_train, X_test, Y_train, Y_test are already defined\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "logistic_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "logistic_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "Y_pred = logistic_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba12879-9215-4a78-ab05-30e5e9c33bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Impute missing values in Y_train\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "Y_train_imputed = imputer.fit_transform(Y_train.values.reshape(-1, 1))\n",
    "Y_train = Y_train_imputed.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd15b4-923a-4770-8c95-54e612d37498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values in Y_train\n",
    "X_train = X_train.dropna(subset=['LoanOnCard'])\n",
    "Y_train = Y_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c68408a-8172-4583-80c8-ce1b469ca68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2.3 d - Print evaluation metrics for the model and clearly share insights.\n",
    "# Check the column names of the DataFrame\n",
    "print(merged_df.columns)\n",
    "\n",
    "# Assuming the target variable is named differently, use the correct column name\n",
    "target_variable = 'LoanOnCard'  # Replace 'TargetVariableName' with the actual name of our target variable\n",
    "\n",
    "# Split the data into features (X) and the target variable (Y)\n",
    "X = merged_df.drop(target_variable, axis=1)  # Features\n",
    "Y = merged_df[target_variable]  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "logistic_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "logistic_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "Y_pred = logistic_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723c7ee0-402c-4472-8467-5ffc24ce1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Initialize SimpleImputer to impute missing values with the most frequent value\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Impute missing values in Y_train\n",
    "Y_train_imputed = imputer.fit_transform(Y_train.values.reshape(-1, 1))\n",
    "Y_train = Y_train_imputed.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae91157-1535-4323-ae72-7bb110bf722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values from X_train and Y_train\n",
    "merged_df.dropna(subset=['LoanOnCard'], inplace=True)\n",
    "\n",
    "# Split the data into features (X) and the target variable (Y)\n",
    "X = merged_df.drop('LoanOnCard', axis=1)  # Features\n",
    "Y = merged_df['LoanOnCard']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26339784-d90b-4b27-ad8f-492c18edcca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "precision = precision_score(Y_test, Y_pred)\n",
    "recall = recall_score(Y_test, Y_pred)\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad11a790-2d3c-4605-8ee8-10107b6e1c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2.3 e - Balance the data using the right balancing technique.\n",
    "# i. Check distribution of the target variable\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b6675f-b272-4141-9aaf-a51d239a4446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to balance the data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, Y_train_balanced = smote.fit_resample(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf639156-2668-4cd2-b68f-1818b3117ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check distribution of the target variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=Y_train)\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.xlabel('Target Variable')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d1b14-acf2-4a08-8740-567920397a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to balance the data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, Y_train_balanced = smote.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436abc7-ba62-414a-9ba2-fe4b1e2df488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii.Say output is class A : 20% and class B : 80%\n",
    "# To address this class imbalance, We need to consider applying various techniques such as:\n",
    "\n",
    "#     Random Undersampling: Randomly remove samples from the majority class (class B) to balance the class distribution.\n",
    "#     Random Oversampling: Randomly duplicate samples from the minority class (class A) to balance the class distribution.\n",
    "#     SMOTE (Synthetic Minority Over-sampling Technique): Generate synthetic samples from the minority class (class A) to balance the class distribution.\n",
    "\n",
    "# The choice of balancing technique depends on the characteristics of dataset and the specific requirements of the problem.\n",
    "\n",
    "# Once we have balanced the classes, we can proceed with training classification model to avoid bias towards the majority class. Remember to evaluate the model's performance using appropriate metrics to ensure that it generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722586f-9d52-4441-885f-bc08c03d6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii.Here you need to balance the target variable as 50:50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7851da-4dbd-400e-8551-37e08ce6b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming Y_train contains the target variable\n",
    "# Check the class distribution before balancing\n",
    "class_distribution_before = Y_train.value_counts()\n",
    "print(\"Class Distribution Before Balancing:\")\n",
    "print(class_distribution_before)\n",
    "\n",
    "# Random Undersampling\n",
    "class_count_min = min(class_distribution_before)\n",
    "Y_train_balanced_undersampled = pd.concat([Y_train[Y_train == cls].sample(class_count_min, replace=False) for cls in class_distribution_before.index])\n",
    "X_train_balanced_undersampled = X_train.loc[Y_train_balanced_undersampled.index]\n",
    "\n",
    "# Random Oversampling\n",
    "class_count_max = max(class_distribution_before)\n",
    "Y_train_balanced_oversampled = pd.concat([Y_train[Y_train == cls].sample(class_count_max, replace=True) for cls in class_distribution_before.index])\n",
    "X_train_balanced_oversampled = X_train.loc[Y_train_balanced_oversampled.index]\n",
    "\n",
    "# Check the class distribution after balancing\n",
    "class_distribution_after_undersampling = Y_train_balanced_undersampled.value_counts()\n",
    "class_distribution_after_oversampling = Y_train_balanced_oversampled.value_counts()\n",
    "print(\"\\nClass Distribution After Balancing (Random Undersampling):\")\n",
    "print(class_distribution_after_undersampling)\n",
    "print(\"\\nClass Distribution After Balancing (Random Oversampling):\")\n",
    "print(class_distribution_after_oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35376eb2-3d16-420c-92f2-ebfcf3d141b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv.Try appropriate method to achieve the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b6da98-5b70-4308-9a90-b2adbc07836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Check the class distribution before balancing\n",
    "print(\"Class Distribution Before Balancing:\")\n",
    "print(Y_train.value_counts())\n",
    "\n",
    "# Apply SMOTE to balance the data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, Y_train_balanced = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "# Check the class distribution after balancing\n",
    "print(\"\\nClass Distribution After Balancing:\")\n",
    "print(pd.Series(Y_train_balanced).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9dd8a-91c3-4667-903b-4dd20ddc5a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.3 f - Again train the same previous model on balanced data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c6b96-9e91-424a-8cdd-ae5c1f18b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Apply SMOTE to balance the data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, Y_train_balanced = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "logistic_classifier_balanced = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the classifier on the balanced training data\n",
    "logistic_classifier_balanced.fit(X_train_balanced, Y_train_balanced)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "Y_pred_balanced = logistic_classifier_balanced.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy_balanced = accuracy_score(Y_test, Y_pred_balanced)\n",
    "print(\"Accuracy on Balanced Data:\", accuracy_balanced)\n",
    "\n",
    "# Print classification report on the test set\n",
    "print(\"Classification Report on Balanced Data:\")\n",
    "print(classification_report(Y_test, Y_pred_balanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f42a84-8b36-433e-a2cd-9f14de02a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_classifier_balanced = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_classifier_balanced = LogisticRegression(solver='liblinear', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8e0fcf-ea15-4d23-adb3-71c724d5c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2.3 g- Print evaluation metrics and clearly share differences observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2843b-8909-4247-a64c-f23c230ca0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Calculate evaluation metrics for original model\n",
    "accuracy_original = accuracy_score(Y_test, Y_pred)\n",
    "precision_original = precision_score(Y_test, Y_pred)\n",
    "recall_original = recall_score(Y_test, Y_pred)\n",
    "f1_original = f1_score(Y_test, Y_pred)\n",
    "\n",
    "# Print evaluation metrics for original model\n",
    "print(\"Evaluation Metrics for Original Model:\")\n",
    "print(\"Accuracy:\", accuracy_original)\n",
    "print(\"Precision:\", precision_original)\n",
    "print(\"Recall:\", recall_original)\n",
    "print(\"F1 Score:\", f1_original)\n",
    "print(\"\")\n",
    "\n",
    "# Calculate evaluation metrics for model trained on balanced data\n",
    "accuracy_balanced = accuracy_score(Y_test, Y_pred_balanced)\n",
    "precision_balanced = precision_score(Y_test, Y_pred_balanced)\n",
    "recall_balanced = recall_score(Y_test, Y_pred_balanced)\n",
    "f1_balanced = f1_score(Y_test, Y_pred_balanced)\n",
    "\n",
    "# Print evaluation metrics for model trained on balanced data\n",
    "print(\"Evaluation Metrics for Model Trained on Balanced Data:\")\n",
    "print(\"Accuracy:\", accuracy_balanced)\n",
    "print(\"Precision:\", precision_balanced)\n",
    "print(\"Recall:\", recall_balanced)\n",
    "print(\"F1 Score:\", f1_balanced)\n",
    "print(\"\")\n",
    "\n",
    "# Print classification report for original model\n",
    "print(\"Classification Report for Original Model:\")\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(\"\")\n",
    "\n",
    "# Print classification report for model trained on balanced data\n",
    "print(\"Classification Report for Model Trained on Balanced Data:\")\n",
    "print(classification_report(Y_test, Y_pred_balanced))\n",
    "print(\"\")\n",
    "\n",
    "# Print confusion matrix for original model\n",
    "print(\"Confusion Matrix for Original Model:\")\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print(\"\")\n",
    "\n",
    "# Print confusion matrix for model trained on balanced data\n",
    "print(\"Confusion Matrix for Model Trained on Balanced Data:\")\n",
    "print(confusion_matrix(Y_test, Y_pred_balanced))\n",
    "\n",
    "\n",
    "##\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# # Assuming you have y_pred and actual_labels\n",
    "# # Replace actual_labels with your actual labels\n",
    "# # Calculate evaluation metrics\n",
    "# accuracy = accuracy_score(actual_labels, y_pred)\n",
    "# precision = precision_score(actual_labels, y_pred)\n",
    "# recall = recall_score(actual_labels, y_pred)\n",
    "# f1 = f1_score(actual_labels, y_pred)\n",
    "\n",
    "# # Print evaluation metrics\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Recall:\", recall)\n",
    "# print(\"F1 Score:\", f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7600ee49-57d6-4bc6-a07f-172adb04805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of samples in X_train and Y_train\n",
    "print(\"Number of samples in X_train:\", len(X_train))\n",
    "print(\"Number of samples in Y_train:\", len(Y_train))\n",
    "\n",
    "# Check the number of samples in X_test and Y_test\n",
    "print(\"Number of samples in X_test:\", len(X_test))\n",
    "print(\"Number of samples in Y_test:\", len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9119337-e76e-4ef1-abb5-b3205fd9c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Performance Improvement: \n",
    "#Part 2.4 a- Train a base model each for SVM, KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c76d84-d1fe-4fd5-b171-0c7bf3275a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Train SVM classifier\n",
    "svm_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions using SVM classifier\n",
    "Y_pred_svm = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate SVM classifier\n",
    "accuracy_svm = accuracy_score(Y_test, Y_pred_svm)\n",
    "print(\"Accuracy of SVM classifier:\", accuracy_svm)\n",
    "print(\"Classification Report for SVM classifier:\")\n",
    "print(classification_report(Y_test, Y_pred_svm))\n",
    "\n",
    "# Initialize KNN classifier\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# Train KNN classifier\n",
    "knn_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions using KNN classifier\n",
    "Y_pred_knn = knn_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate KNN classifier\n",
    "accuracy_knn = accuracy_score(Y_test, Y_pred_knn)\n",
    "print(\"\\nAccuracy of KNN classifier:\", accuracy_knn)\n",
    "print(\"Classification Report for KNN classifier:\")\n",
    "print(classification_report(Y_test, Y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d7802-1952-4aba-98d5-a3ac2809070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2.4 b - Tune parameters for each of the models wherever required and finalize a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0801f43d-e7c6-4724-af5d-42e75fc0fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_svm = {'C': [0.1, 1, 10],\n",
    "                  'kernel': ['linear', 'rbf'],\n",
    "                  'gamma': [0.1, 1, 10]}\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier_tuned = SVC()\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search_svm = GridSearchCV(svm_classifier_tuned, param_grid_svm, cv=5, scoring='accuracy')\n",
    "grid_search_svm.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_svm = grid_search_svm.best_params_\n",
    "best_score_svm = grid_search_svm.best_score_\n",
    "\n",
    "print(\"Best Parameters for SVM:\", best_params_svm)\n",
    "print(\"Best Score for SVM:\", best_score_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144a793-552f-4294-ad83-cf3cab1934bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Model Tuning\n",
    "# Define the parameter grid\n",
    "param_grid_knn = {'n_neighbors': [3, 5, 7, 9, 11]}\n",
    "\n",
    "# Initialize KNN classifier\n",
    "knn_classifier_tuned = KNeighborsClassifier()\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search_knn = GridSearchCV(knn_classifier_tuned, param_grid_knn, cv=5, scoring='accuracy')\n",
    "grid_search_knn.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_knn = grid_search_knn.best_params_\n",
    "best_score_knn = grid_search_knn.best_score_\n",
    "\n",
    "print(\"Best Parameters for KNN:\", best_params_knn)\n",
    "print(\"Best Score for KNN:\", best_score_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfda08b-444d-428e-9710-08bc1bd4df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizing the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df12d6f-2d3f-4c41-90c8-4ed369b8dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best SVM and KNN classifiers with tuned hyperparameters\n",
    "best_svm_classifier = grid_search_svm.best_estimator_\n",
    "best_knn_classifier = grid_search_knn.best_estimator_\n",
    "\n",
    "# Evaluate performance on test set\n",
    "accuracy_svm_final = best_svm_classifier.score(X_test, Y_test)\n",
    "accuracy_knn_final = best_knn_classifier.score(X_test, Y_test)\n",
    "\n",
    "print(\"Final Accuracy for SVM:\", accuracy_svm_final)\n",
    "print(\"Final Accuracy for KNN:\", accuracy_knn_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c299b-6c5c-4f10-904e-52588409f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2.4 c- Print evaluation metrics for final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45185035-1a2c-4a50-921c-4d015435a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predictions using the best SVM classifier\n",
    "Y_pred_svm_final = best_svm_classifier.predict(X_test)\n",
    "\n",
    "# Predictions using the best KNN classifier\n",
    "Y_pred_knn_final = best_knn_classifier.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics for SVM model\n",
    "print(\"Evaluation Metrics for Final SVM Model:\")\n",
    "print(classification_report(Y_test, Y_pred_svm_final))\n",
    "\n",
    "# Print evaluation metrics for KNN model\n",
    "print(\"\\nEvaluation Metrics for Final KNN Model:\")\n",
    "print(classification_report(Y_test, Y_pred_knn_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e4bf49-80c6-4da6-8216-02b173c5eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2.4 d- Share improvement achieved from base model to final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00760e-4db7-460a-9e2b-53ace835754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate evaluation metrics for base SVM model\n",
    "accuracy_base_svm = accuracy_score(Y_test, Y_pred_svm_base)\n",
    "precision_base_svm = precision_score(Y_test, Y_pred_svm_base)\n",
    "recall_base_svm = recall_score(Y_test, Y_pred_svm_base)\n",
    "f1_base_svm = f1_score(Y_test, Y_pred_svm_base)\n",
    "\n",
    "# Calculate evaluation metrics for base KNN model\n",
    "accuracy_base_knn = accuracy_score(Y_test, Y_pred_knn_base)\n",
    "precision_base_knn = precision_score(Y_test, Y_pred_knn_base)\n",
    "recall_base_knn = recall_score(Y_test, Y_pred_knn_base)\n",
    "f1_base_knn = f1_score(Y_test, Y_pred_knn_base)\n",
    "\n",
    "# Calculate evaluation metrics for final SVM model\n",
    "accuracy_final_svm = accuracy_score(Y_test, Y_pred_svm_final)\n",
    "precision_final_svm = precision_score(Y_test, Y_pred_svm_final)\n",
    "recall_final_svm = recall_score(Y_test, Y_pred_svm_final)\n",
    "f1_final_svm = f1_score(Y_test, Y_pred_svm_final)\n",
    "\n",
    "# Calculate evaluation metrics for final KNN model\n",
    "accuracy_final_knn = accuracy_score(Y_test, Y_pred_knn_final)\n",
    "precision_final_knn = precision_score(Y_test, Y_pred_knn_final)\n",
    "recall_final_knn = recall_score(Y_test, Y_pred_knn_final)\n",
    "f1_final_knn = f1_score(Y_test, Y_pred_knn_final)\n",
    "\n",
    "# Calculate improvements for SVM model\n",
    "accuracy_improvement_svm = accuracy_final_svm - accuracy_base_svm\n",
    "precision_improvement_svm = precision_final_svm - precision_base_svm\n",
    "recall_improvement_svm = recall_final_svm - recall_base_svm\n",
    "f1_improvement_svm = f1_final_svm - f1_base_svm\n",
    "\n",
    "# Calculate improvements for KNN model\n",
    "accuracy_improvement_knn = accuracy_final_knn - accuracy_base_knn\n",
    "precision_improvement_knn = precision_final_knn - precision_base_knn\n",
    "recall_improvement_knn = recall_final_knn - recall_base_knn\n",
    "f1_improvement_knn = f1_final_knn - f1_base_knn\n",
    "\n",
    "# Print improvements for SVM model\n",
    "print(\"Improvement Achieved from Base to Final SVM Model:\")\n",
    "print(\"Accuracy Improvement:\", accuracy_improvement_svm)\n",
    "print(\"Precision Improvement:\", precision_improvement_svm)\n",
    "print(\"Recall Improvement:\", recall_improvement_svm)\n",
    "print(\"F1-score Improvement:\", f1_improvement_svm)\n",
    "\n",
    "# Print improvements for KNN model\n",
    "print(\"\\nImprovement Achieved from Base to Final KNN Model:\")\n",
    "print(\"Accuracy Improvement:\", accuracy_improvement_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b069ff24-90bb-45c2-8472-e96a8050a19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predictions using the base SVM model\n",
    "Y_pred_svm_base = svm_classifier.predict(X_test)\n",
    "\n",
    "# Predictions using the base KNN model\n",
    "Y_pred_knn_base = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics for base SVM model\n",
    "accuracy_base_svm = accuracy_score(Y_test, Y_pred_svm_base)\n",
    "precision_base_svm = precision_score(Y_test, Y_pred_svm_base)\n",
    "recall_base_svm = recall_score(Y_test, Y_pred_svm_base)\n",
    "f1_base_svm = f1_score(Y_test, Y_pred_svm_base)\n",
    "\n",
    "# Calculate evaluation metrics for base KNN model\n",
    "accuracy_base_knn = accuracy_score(Y_test, Y_pred_knn_base)\n",
    "precision_base_knn = precision_score(Y_test, Y_pred_knn_base)\n",
    "recall_base_knn = recall_score(Y_test, Y_pred_knn_base)\n",
    "f1_base_knn = f1_score(Y_test, Y_pred_knn_base)\n",
    "\n",
    "# Calculate evaluation metrics for final SVM model\n",
    "accuracy_final_svm = accuracy_score(Y_test, Y_pred_svm_final)\n",
    "precision_final_svm = precision_score(Y_test, Y_pred_svm_final)\n",
    "recall_final_svm = recall_score(Y_test, Y_pred_svm_final)\n",
    "f1_final_svm = f1_score(Y_test, Y_pred_svm_final)\n",
    "\n",
    "# Calculate evaluation metrics for final KNN model\n",
    "accuracy_final_knn = accuracy_score(Y_test, Y_pred_knn_final)\n",
    "precision_final_knn = precision_score(Y_test, Y_pred_knn_final)\n",
    "recall_final_knn = recall_score(Y_test, Y_pred_knn_final)\n",
    "f1_final_knn = f1_score(Y_test, Y_pred_knn_final)\n",
    "\n",
    "# Calculate improvements for SVM model\n",
    "accuracy_improvement_svm = accuracy_final_svm - accuracy_base_svm\n",
    "precision_improvement_svm = precision_final_svm - precision_base_svm\n",
    "recall_improvement_svm = recall_final_svm - recall_base_svm\n",
    "f1_improvement_svm = f1_final_svm - f1_base_svm\n",
    "\n",
    "# Calculate improvements for KNN model\n",
    "accuracy_improvement_knn = accuracy_final_knn - accuracy_base_knn\n",
    "precision_improvement_knn = precision_final_knn - precision_base_knn\n",
    "recall_improvement_knn = recall_final_knn - recall_base_knn\n",
    "f1_improvement_knn = f1_final_knn - f1_base_knn\n",
    "\n",
    "# Print improvements for SVM model\n",
    "print(\"Improvement Achieved from Base to Final SVM Model:\")\n",
    "print(\"Accuracy Improvement:\", accuracy_improvement_svm)\n",
    "print(\"Precision Improvement:\", precision_improvement_svm)\n",
    "print(\"Recall Improvement:\", recall_improvement_svm)\n",
    "print(\"F1-score Improvement:\", f1_improvement_svm)\n",
    "\n",
    "# Print improvements for KNN model\n",
    "print(\"\\nImprovement Achieved from Base to Final KNN Model:\")\n",
    "print(\"Accuracy Improvement:\", accuracy_improvement_knn)\n",
    "print(\"Precision Improvement:\", precision_improvement_knn)\n",
    "print(\"Recall Improvement:\", recall_improvement_knn)\n",
    "print(\"F1-score Improvement:\", f1_improvement_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9460cc79-dc17-4c3b-bfcf-eee3d6418a39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
